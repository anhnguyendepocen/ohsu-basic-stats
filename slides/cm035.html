<!DOCTYPE html>
<html>
  <head>
    <title>CONJ620: CM 3.2</title>
    <meta charset="utf-8">
    <meta name="author" content="Alison Hill" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/ohsu.css" type="text/css" />
    <link rel="stylesheet" href="css/ohsu-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# CONJ620: CM 3.2
## Sampling, Simulation, &amp; Bootstrapping
### Alison Hill

---




## Family of *t*-tests

- One-sample *t*-test
- Independent samples *t*-test
- Dependent samples *t*-test (also known as paired) 

---
## What do they all have in common?

We want to compare two things- mean 1 and mean 2!

---
## Ways to get independent samples

* Random assignment of participants to two different experimental conditions 
  * Scream versus Scream 2
  * Classical music versus silence
* Naturally occurring assignment of participants to two different groups 
  * Male versus female
  * Young versus old
  
---
## Formula for the independent groups *t*-test

`$$t=\frac{(\bar{y}_1-\bar{y}_2)-(\mu_1-\mu_2)}{SE}$$`

Generally, 
&gt; `\(H_0: \mu_1-\mu_2=0\)` and `\(H_1: \mu_1-\mu_2\neq0\)`

So `\(\mu_1-\mu_2\)` is often excluded from the formula. Since we now have two sample means and therefore two sample variances, we need some way to combine these two variances in a logical way. The answer is to __pool__ the variances to estimate the SE of the difference, `\((\bar{y}_1-\bar{y}_2)\)`:
`$$s_{\bar{y}_1-\bar{y}_2}=s_{pooled}\times{\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}$$`
and `\(s_{pooled}\)` is:
`$$s_{pooled}=\sqrt{\frac{(n_1-1)s_{y_1}^2+(n_2-1)s_{y_2}^2}{n_1+n_2-2}}$$`
---
## The *t*-test as a general linear model (GLM) 

All statistical procedures are basically the same thing:
`$$outcome_i=(model)+error_i$$`

In my simple linear regression from the past few labs, this has been:
`$$prestige_i=(intercept + education_i)+error_i$$`

Or more generally:
`$$y_i=(b_0 + b_1x_i)+error_i$$`

&gt; __N.B.__ `\(Error_i\)` is an unknowable, incalculable statistic- it is the deviation of the `\(i^{th}\)` value from the (unobservable) true value. You can think of it as measurement error. This is different from the error of the regression formula, which is defined as the *residual* of the `\(i^{th}\)` value and calculated as the difference between the observed `\((y_i)\)` and predicted scores `\((\hat{y_i})\)`. 

---
## Sidebar: The GLM in matrix notation
The GLM we have been dealing with thus far includes just one independent variable and thus just one `\(b_1\)`. However, the full GLM is better represented as a matrix

`$$\boldsymbol{Y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}$$`

where...

* `\(\boldsymbol{Y}\)` is the *response vector* of length *N*;

* `\(\boldsymbol{\epsilon}\)` is the *error vector* of length *N*;

* `\(\boldsymbol{\beta}\)` is the vector of parameters of length *p*+1 where *p* is the number of IVs and the 1 accounts for the intercept;

* and `\(\boldsymbol{X}\)` is called the *design matrix* consisting of a matrix of *N* rows and *p+1* columns

---
## Design matrices with one independent variable

.pull-left[

In both simple linear regression and the independent samples *t*-test, `\(\boldsymbol{X}\)` is a matrix of *N* rows and *2* columns. Note that the number of columns in `\(\boldsymbol{X}\)` must always equal the number of rows in `\(\boldsymbol{\beta}\)`.

\[ \begin{bmatrix}
Y_1 \\
Y_2 \\
\vdots \\
Y_n \\
\end{bmatrix} = \begin{bmatrix}
1 &amp; X_1 \\
1 &amp; X_2 \\
\vdots &amp; \vdots \\
1 &amp; X_n \\
\end{bmatrix} \begin{bmatrix}
\beta_0 \\
\beta_1 \\
\end{bmatrix} + \begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\
\vdots \\
\epsilon_n \\
\end{bmatrix} \]

]

.pull-right[

In simple linear regression, the vector *X* can take on any value. In the independent samples *t*-test, this vector simply contains 0's and 1's. Below, *n*=4 with 2 in each group to illustrate.

\[ \begin{bmatrix}
Y_1 \\
Y_2 \\
Y_3 \\
Y_4 \\
\end{bmatrix} = \begin{bmatrix}
1 &amp; 0 \\
1 &amp; 0 \\
1 &amp; 1 \\
1 &amp; 1 \\
\end{bmatrix} \begin{bmatrix}
\beta_0 \\
\beta_1 \\
\end{bmatrix} + \begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\
\epsilon_3 \\
\epsilon_4 \\
\end{bmatrix} \]
]
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "atelier-lakeside-light",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
